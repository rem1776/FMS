<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FMS: mpp_mod</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">FMS
   &#160;<span id="projectnumber">2021.01-dev</span>
   </div>
   <div id="projectbrief">Flexible Modeling System</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('group__mpp__mod.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Data Types</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">mpp_mod<div class="ingroups"><a class="el" href="group__mpp.html">MPP</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>A set of simple calls to provide a uniform interface to different message-passing libraries. It currently can be implemented either in the SGI/Cray native SHMEM library or in the MPI standard. Other libraries (e.g MPI-2, Co-Array Fortran) can be incorporated as the need arises.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ga8c33cfdab671e1afa5b95acec1d1e5b1"><td class="memItemLeft" align="right" valign="top"><a id="ga8c33cfdab671e1afa5b95acec1d1e5b1"></a>
logical, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_record_timing_data</b> =.TRUE.</td></tr>
<tr class="separator:ga8c33cfdab671e1afa5b95acec1d1e5b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6624d98bb8dc8289e1a76ee2b0017ec9"><td class="memItemLeft" align="right" valign="top"><a id="ga6624d98bb8dc8289e1a76ee2b0017ec9"></a>
type(mpp_type), target, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_byte</b></td></tr>
<tr class="separator:ga6624d98bb8dc8289e1a76ee2b0017ec9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga90a613436bf4bba787026aeecd058f8d"><td class="memItemLeft" align="right" valign="top"><a id="ga90a613436bf4bba787026aeecd058f8d"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_full_init</b> = -1</td></tr>
<tr class="separator:ga90a613436bf4bba787026aeecd058f8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadd4b4f63882e1ead523312aef7f430c5"><td class="memItemLeft" align="right" valign="top"><a id="gadd4b4f63882e1ead523312aef7f430c5"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_init_true_only</b> = 0</td></tr>
<tr class="separator:gadd4b4f63882e1ead523312aef7f430c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga31a5e760356d94c51837b8ed3bfbcbef"><td class="memItemLeft" align="right" valign="top"><a id="ga31a5e760356d94c51837b8ed3bfbcbef"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_peset_allocated</b> = 1</td></tr>
<tr class="separator:ga31a5e760356d94c51837b8ed3bfbcbef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaa4fd1673a2cff1568662140d29ed1f3a"><td class="memItemLeft" align="right" valign="top"><a id="gaa4fd1673a2cff1568662140d29ed1f3a"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_clocks_init</b> = 2</td></tr>
<tr class="separator:gaa4fd1673a2cff1568662140d29ed1f3a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1d0c0ca5f8c2a2769139ad6b2c4ce497"><td class="memItemLeft" align="right" valign="top"><a id="ga1d0c0ca5f8c2a2769139ad6b2c4ce497"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_datatype_list_init</b> = 3</td></tr>
<tr class="separator:ga1d0c0ca5f8c2a2769139ad6b2c4ce497"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaa1b17ca8452e65f0a3d31206cf64be3b"><td class="memItemLeft" align="right" valign="top"><a id="gaa1b17ca8452e65f0a3d31206cf64be3b"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_logfile_init</b> = 4</td></tr>
<tr class="separator:gaa1b17ca8452e65f0a3d31206cf64be3b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga42760edcf1874ac719277cb860273eee"><td class="memItemLeft" align="right" valign="top"><a id="ga42760edcf1874ac719277cb860273eee"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_read_namelist</b> = 5</td></tr>
<tr class="separator:ga42760edcf1874ac719277cb860273eee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga442c5ba6ae0025615fe70694802e2809"><td class="memItemLeft" align="right" valign="top"><a id="ga442c5ba6ae0025615fe70694802e2809"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_etc_unit</b> = 6</td></tr>
<tr class="separator:ga442c5ba6ae0025615fe70694802e2809"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga44fff69706d0583a0d1eb338a59059de"><td class="memItemLeft" align="right" valign="top"><a id="ga44fff69706d0583a0d1eb338a59059de"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>mpp_init_test_requests_allocated</b> = 7</td></tr>
<tr class="separator:ga44fff69706d0583a0d1eb338a59059de"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf0b9d6445f103d775b624ed20446e3e9"><td class="memItemLeft" align="right" valign="top"><a id="gaf0b9d6445f103d775b624ed20446e3e9"></a>
integer, parameter, public&#160;</td><td class="memItemRight" valign="bottom"><b>input_str_length</b> = 256</td></tr>
<tr class="separator:gaf0b9d6445f103d775b624ed20446e3e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9f26741b5ef128645b1a21cd1c875d25"><td class="memItemLeft" align="right" valign="top"><a id="ga9f26741b5ef128645b1a21cd1c875d25"></a>
character(len=:), dimension(:), allocatable, target, public&#160;</td><td class="memItemRight" valign="bottom"><b>input_nml_file</b></td></tr>
<tr class="separator:ga9f26741b5ef128645b1a21cd1c875d25"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>A set of simple calls to provide a uniform interface to different message-passing libraries. It currently can be implemented either in the SGI/Cray native SHMEM library or in the MPI standard. Other libraries (e.g MPI-2, Co-Array Fortran) can be incorporated as the need arises. </p>
<dl class="section author"><dt>Author</dt><dd>V. Balaji &lt;"V.Balaji@noaa.gov"&gt;</dd></dl>
<p>The data transfer between a processor and its own memory is based on <code>load</code> and <code>store</code> operations upon memory. Shared-memory systems (including distributed shared memory systems) have a single address space and any processor can acquire any data within the memory by <code>load</code> and <code>store</code>. The situation is different for distributed parallel systems. Specialized MPP systems such as the T3E can simulate shared-memory by direct data acquisition from remote memory. But if the parallel code is distributed across a cluster, or across the Net, messages must be sent and received using the protocols for long-distance communication, such as TCP/IP. This requires a `&lsquo;handshaking&rsquo;' between nodes of the distributed system. One can think of the two different methods as involving <code>put</code>s or <code>get</code>s (e.g the SHMEM library), or in the case of negotiated communication (e.g MPI), <code>send</code>s and <code>recv</code>s.</p>
<p>The difference between SHMEM and MPI is that SHMEM uses one-sided communication, which can have very low-latency high-bandwidth implementations on tightly coupled systems. MPI is a standard developed for distributed computing across loosely-coupled systems, and therefore incurs a software penalty for negotiating the communication. It is however an open industry standard whereas SHMEM is a proprietary interface. Besides, the <code>put</code>s or <code>get</code>s on which it is based cannot currently be implemented in a cluster environment (there are recent announcements from Compaq that occasion hope).</p>
<p>The message-passing requirements of climate and weather codes can be reduced to a fairly simple minimal set, which is easily implemented in any message-passing API. <code>mpp_mod</code> provides this API.</p>
<p>Features of <code>mpp_mod</code> include: </p><ol>
<li>
Simple, minimal API, with free access to underlying API for  more complicated stuff.<br  />
 </li>
<li>
Design toward typical use in climate/weather CFD codes.  </li>
<li>
Performance to be not significantly lower than any native API.  </li>
</ol>
<p>This module is used to develop higher-level calls for &lt;LINK
   SRC="mpp_domains.html"&gt;domain decomposition&lt;/LINK&gt; and &lt;LINK
   SRC="mpp_io.html"&gt;parallel I/O&lt;/LINK&gt;. <br  />
 Parallel computing is initially daunting, but it soon becomes second nature, much the way many of us can now write vector code without much effort. The key insight required while reading and writing parallel code is in arriving at a mental grasp of several independent parallel execution streams through the same code (the SPMD model). Each variable you examine may have different values for each stream, the processor ID being an obvious example. Subroutines and function calls are particularly subtle, since it is not always obvious from looking at a call what synchronization between execution streams it implies. An example of erroneous code would be a global barrier call (see &lt;LINK SRC="#mpp_sync"&gt;mpp_sync&lt;/LINK&gt; below) placed within a code block that not all PEs will execute, e.g:</p>
<pre>
   if( pe.EQ.0 )call mpp_sync()
   </pre><p>Here only PE 0 reaches the barrier, where it will wait indefinitely. While this is a particularly egregious example to illustrate the coding flaw, more subtle versions of the same are among the most common errors in parallel code. <br  />
 It is therefore important to be conscious of the context of a subroutine or function call, and the implied synchronization. There are certain calls here (e.g <code>mpp_declare_pelist, mpp_init, mpp_set_stack_size</code>) which must be called by all PEs. There are others which must be called by a subset of PEs (here called a <code>pelist</code>) which must be called by all the PEs in the <code>pelist</code> (e.g <code>mpp_max, mpp_sum, mpp_sync</code>). Still others imply no synchronization at all. I will make every effort to highlight the context of each call in the MPP modules, so that the implicit synchronization is spelt out. <br  />
 For performance it is necessary to keep synchronization as limited as the algorithm being implemented will allow. For instance, a single message between two PEs should only imply synchronization across the PEs in question. A <em>global</em> synchronization (or <em>barrier</em>) is likely to be slow, and is best avoided. But codes first parallelized on a Cray T3E tend to have many global syncs, as very fast barriers were implemented there in hardware. <br  />
 Another reason to use pelists is to run a single program in MPMD mode, where different PE subsets work on different portions of the code. A typical example is to assign an ocean model and atmosphere model to different PE subsets, and couple them concurrently instead of running them serially. The MPP module provides the notion of a <em>current pelist</em>, which is set when a group of PEs branch off into a subset. Subsequent calls that omit the <code>pelist</code> optional argument (seen below in many of the individual calls) assume that the implied synchronization is across the current pelist. The calls <code>mpp_root_pe</code> and <code>mpp_npes</code> also return the values appropriate to the current pelist. The <code>mpp_set_current_pelist</code> call is provided to set the current pelist.  <br  />
 &lt;PUBLIC&gt; F90 is a strictly-typed language, and the syntax pass of the compiler requires matching of type, kind and rank (TKR). Most calls listed here use a generic type, shown here as <code>MPP_TYPE_</code>. This is resolved in the pre-processor stage to any of a variety of types. In general the MPP operations work on 4-byte and 8-byte variants of <code>integer, real, complex, logical</code> variables, of rank 0 to 5, leading to 48 specific module procedures under the same generic interface. Any of the variables below shown as <code>MPP_TYPE_</code> is treated in this way. &lt;/PUBLIC&gt; </p>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FMS: mpp_domains_mod</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">FMS
   &#160;<span id="projectnumber">2021.01-dev</span>
   </div>
   <div id="projectbrief">Flexible Modeling System</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('group__mpp__domains__mod.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Data Types</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">mpp_domains_mod<div class="ingroups"><a class="el" href="group__mpp.html">MPP</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>Domain decomposition and domain update for message-passing codes.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ga0406f210136aebd26e0c9ad46e4e6912"><td class="memItemLeft" align="right" valign="top"><a id="ga0406f210136aebd26e0c9ad46e4e6912"></a>
type(domain1d), save, public&#160;</td><td class="memItemRight" valign="bottom"><b>null_domain1d</b></td></tr>
<tr class="separator:ga0406f210136aebd26e0c9ad46e4e6912"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga14f594ff34345eb378085e2d2474a561"><td class="memItemLeft" align="right" valign="top"><a id="ga14f594ff34345eb378085e2d2474a561"></a>
type(domain2d), save, public&#160;</td><td class="memItemRight" valign="bottom"><b>null_domain2d</b></td></tr>
<tr class="separator:ga14f594ff34345eb378085e2d2474a561"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4b2cdd4e8b26c3a979ac96878d114c5d"><td class="memItemLeft" align="right" valign="top"><a id="ga4b2cdd4e8b26c3a979ac96878d114c5d"></a>
type(domainug), save, public&#160;</td><td class="memItemRight" valign="bottom"><b>null_domainug</b></td></tr>
<tr class="separator:ga4b2cdd4e8b26c3a979ac96878d114c5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>Domain decomposition and domain update for message-passing codes. </p>
<dl class="section author"><dt>Author</dt><dd>V. Balaji SGI/GFDL Princeton University</dd></dl>
<p>A set of simple calls for domain decomposition and domain updates on rectilinear grids. It requires the module <a class="el" href="mpp_8F90.html" title="File for mpp_mod.">mpp.F90</a>, upon which it is built.<br  />
 Scalable implementations of finite-difference codes are generally based on decomposing the model domain into subdomains that are distributed among processors. These domains will then be obliged to exchange data at their boundaries if data dependencies are merely nearest-neighbour, or may need to acquire information from the global domain if there are extended data dependencies, as in the spectral transform. The domain decomposition is a key operation in the development of parallel codes.<br  />
<br  />
 mpp_domains_mod provides a domain decomposition and domain update API for rectilinear grids, built on top of the mpp_mod API for message passing. Features of mpp_domains_mod include:<br  />
<br  />
 Simple, minimal API, with free access to underlying API for more complicated stuff.<br  />
<br  />
 Design toward typical use in climate/weather CFD codes.<br  />
 </p><dl class="section user"><dt>[Domains]</dt><dd>It is assumed that domain decomposition will mainly be in 2 horizontal dimensions, which will in general be the two fastest-varying indices. There is a separate implementation of 1D decomposition on the fastest-varying index, and 1D decomposition on the second index, treated as a special case of 2D decomposition, is also possible. We define domain as the grid associated with a <em>task</em>. We define the compute domain as the set of gridpoints that are computed by a task, and the data domain as the set of points that are required by the task for the calculation. There can in general be more than 1 task per PE, though often the number of domains is the same as the processor count. We define the global domain as the global computational domain of the entire model (i.e, the same as the computational domain if run on a single processor). 2D domains are defined using a derived type domain2D, constructed as follows (see comments in code for more details). <pre class="fragment"> type, public :: domain_axis_spec\n
   private\n
   integer :: begin, end, size, max_size\n
   logical :: is_global\n
 end type domain_axis_spec\n

 type, public :: domain1D\n
   private\n
   type(domain_axis_spec) :: compute, data, global, active\n
   logical :: mustputb, mustgetb, mustputf, mustgetf, folded\n
   type(domain1D), pointer, dimension(:) :: list\n
   integer :: pe  ! pe to which the domain is assigned\n
   integer :: pos\n
 end type domain1D

 type, public :: domain2D\n
   private\n
   type(domain1D) :: x\n
   type(domain1D) :: y\n
   type(domain2D), pointer, dimension(:) :: list\n
   integer :: pe ! PE to which this domain is assigned\n
   integer :: pos\n
 end type domain2D\n

 type(domain1D), public :: NULL_DOMAIN1D\n
 type(domain2D), public :: NULL_DOMAIN2D\n
</pre> </dd></dl>


<h3><a id="mpp_define_layout"></a>mpp_define_layout Interface</h3><div class="textblock">Retrieve layout associated with a domain decomposition. Given a global 2D domain and the number of divisions in the decomposition ndivs (usually the PE count unless some domains are <em>masked</em>) this calls returns a 2D domain layout. By default, mpp_define_layout will attempt to divide the 2D index space into domains that maintain the aspect ratio of the global domain. If this cannot be done, the algorithm favours domains that are longer in <em>x</em> than <em>y</em>, a preference that could improve vector performance. <br  />
Example usage: call mpp_define_layout( global_indices, ndivs, layout ) </div>

<h3><a id="mpp_define_domains"></a>mpp_define_domains Interface</h3><div class="textblock">Set up a domain decomposition.There are two forms for the <em>mpp_define_domains</em> call. The 2D version is generally to be used but is built by repeated calls to the 1D version, also provided.<br  />
Example usage: call mpp_define_domains( global_indices, ndivs, domain, &amp; pelist, flags, halo, extent, maskmap ) call mpp_define_domains( global_indices, layout, domain, pelist, &amp; xflags, yflags, xhalo, yhalo, &amp; xextent, yextent, maskmap, name ) </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir"></td><td class="paramname">global_indices</td><td>Defines the global domain. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">ndivs</td><td>The number of domain divisions required. </td></tr>
    <tr><td class="paramdir">[in,out]</td><td class="paramname">domain</td><td>Holds the resulting domain decomposition. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">pelist</td><td>List of PEs to which the domains are to be assigned. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">flags</td><td>An optional flag to pass additional information about the desired domain topology. Useful flags in a 1D decomposition include <code>GLOBAL_DATA_DOMAIN</code> and <code>CYCLIC_GLOBAL_DOMAIN</code>. Flags are integers: multiple flags may be added together. The flag values are public parameters available by use association. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">halo</td><td>Width of the halo. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">extent</td><td>Normally <code>mpp_define_domains</code> attempts an even division of the global domain across <code>ndivs</code> domains. The <code>extent</code> array can be used by the user to pass a custom domain division. The <code>extent</code> array has <code>ndivs</code> elements and holds the compute domain widths, which should add up to cover the global domain exactly. </td></tr>
    <tr><td class="paramdir"></td><td class="paramname">maskmap</td><td>Some divisions may be masked (<code>maskmap=.FALSE.</code>) to exclude them from the computation (e.g for ocean model domains that are all land). The <code>maskmap</code> array is dimensioned <code>ndivs</code> and contains <code>.TRUE.</code> values for any domain that must be <em>included</em> in the computation (default all). The <code>pelist</code> array length should match the number of domains included in the computation.</td></tr>
  </table>
  </dd>
</dl>
<p><br  />
Example usage: call mpp_define_domains( (/1,100/), 10, domain, &amp; flags=GLOBAL_DATA_DOMAIN+CYCLIC_GLOBAL_DOMAIN, halo=2 )defines 10 compute domains spanning the range [1,100] of the global domain. The compute domains are non-overlapping blocks of 10. All the data domains are global, and with a halo of 2 span the range [-1:102]. And since the global domain has been declared to be cyclic, domain(9)next =&gt; domain(0) and domain(0)prev =&gt; domain(9). A field is allocated on the data domain, and computations proceed on the compute domain. A call to mpp_update_domains would fill in the values in the halo region:<br  />
 <br  />
 call mpp_get_data_domain( domain, isd, ied ) !returns -1 and 102 call mpp_get_compute_domain( domain, is, ie ) !returns (1,10) on PE 0 ... allocate( a(isd:ied) ) do i = is,ie a(i) = &lt;perform computations&gt; end do call mpp_update_domains( a, domain )<br  />
 <br  />
 The call to mpp_update_domainsfills in the regions outside the compute domain. Since the global domain is cyclic, the values at <em>i=</em>(-1,0) are the same as at <em>i=</em>(99,100); and <em>i=</em>(101,102) are the same as <em>i=</em>(1,2).The 2D version is just an extension of this syntax to two dimensions.The 2D version of the above should generally be used in codes, including 1D-decomposed ones, if there is a possibility of future evolution toward 2D decomposition. The arguments are similar to the 1D case, except that now we have optional arguments flags, halo, extent and maskmap along two axes.flags can now take an additional possible value to fold one or more edges. This is done by using flags <em>FOLD_WEST_EDGE</em>, <em>FOLD_EAST_EDGE</em>, <em>FOLD_SOUTH_EDGE</em> or <em>FOLD_NORTH_EDGE</em>. When a fold exists (e.g cylindrical domain), vector fields reverse sign upon crossing the fold. This parity reversal is performed only in the vector version of mpp_update_domains. In addition, shift operations may need to be applied to vector fields on staggered grids, also described in the vector interface to mpp_update_domains.<code>name</code> is the name associated with the decomposition, e.g <code>'Ocean model'</code>. If this argument is present, <code>mpp_define_domains</code> will print the domain decomposition generated to <code>stdlog</code>.<br  />
Examples: call mpp_define_domains( (/1,100,1,100/), (/2,2/), domain, xhalo=1 ) will create the following domain layout:<br  />
 <br  />
 |------&mdash;|--------&mdash;|--------&mdash;|----------&mdash;| |domain(1)|domain(2) |domain(3) |domain(4) | |-----------&mdash;|------&mdash;|--------&mdash;|--------&mdash;|----------&mdash;| </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Compute domain </th><th class="markdownTableHeadNone">1,50,1,50 </th><th class="markdownTableHeadNone">51,100,1,50 </th><th class="markdownTableHeadNone">1,50,51,100 </th><th class="markdownTableHeadNone">51,100,51,100  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Data domain </td><td class="markdownTableBodyNone">0,51,1,50 </td><td class="markdownTableBodyNone">50,101,1,50 </td><td class="markdownTableBodyNone">0,51,51,100 </td><td class="markdownTableBodyNone">50,101,51,100  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">-----------&mdash; </td><td class="markdownTableBodyNone">------&mdash; </td><td class="markdownTableBodyNone">--------&mdash; </td><td class="markdownTableBodyNone">--------&mdash; </td><td class="markdownTableBodyNone">----------&mdash;  </td></tr>
</table>
Again, we allocate arrays on the data domain, perform computations on the compute domain, and call mpp_update_domains to update the halo region.If we wished to perfom a 1D decomposition along Y on the same global domain, we could use: call mpp_define_domains( (/1,100,1,100/), layout=(/4,1/), domain, xhalo=1 ) This will create the following domain layout:<br  />
 <br  />
 |-------&mdash;|--------&mdash;|--------&mdash;|---------&mdash;| |domain(1) |domain(2) |domain(3) |domain(4) | |-----------&mdash;|-------&mdash;|--------&mdash;|--------&mdash;|---------&mdash;| </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Compute domain </th><th class="markdownTableHeadNone">1,100,1,25 </th><th class="markdownTableHeadNone">1,100,26,50 </th><th class="markdownTableHeadNone">1,100,51,75 </th><th class="markdownTableHeadNone">1,100,76,100  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Data domain </td><td class="markdownTableBodyNone">0,101,1,25 </td><td class="markdownTableBodyNone">0,101,26,50 </td><td class="markdownTableBodyNone">0,101,51,75 </td><td class="markdownTableBodyNone">1,101,76,100  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">-----------&mdash; </td><td class="markdownTableBodyNone">-------&mdash; </td><td class="markdownTableBodyNone">--------&mdash; </td><td class="markdownTableBodyNone">--------&mdash; </td><td class="markdownTableBodyNone">---------&mdash;  </td></tr>
</table>
</div>

<h3><a id="mpp_define_null_domain"></a>mpp_define_null_domain Interface</h3><div class="textblock">Defines a nullified 1D or 2D domain </div>

<h3><a id="mpp_copy_domain"></a>mpp_copy_domain Interface</h3><div class="textblock">Copy 1D or 2D domain </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">domain_in</td><td>Input domain to get read </td></tr>
    <tr><td class="paramname">domain_out</td><td>Output domain to get written to </td></tr>
  </table>
  </dd>
</dl>
</div>

<h3><a id="mpp_deallocate_domain"></a>mpp_deallocate_domain Interface</h3><div class="textblock">Deallocate given 1D or 2D domain </div>

<h3><a id="mpp_modify_domain"></a>mpp_modify_domain Interface</h3><div class="textblock">Modifies the extents (compute, data and global) of a given domain </div>

<h3><a id="mpp_update_domains"></a>mpp_update_domains Interface</h3><div class="textblock">Performs halo updates for a given domain.<br  />
Used to perform a halo update of a domain-decomposed array on each PE. <em>MPP_TYPE</em> can be of type complex, integer, logical or real of 4-byte or 8-byte kind; of rank up to 5. The vector version (with two input data fields) is only present for real types. For 2D domain updates, if there are halos present along both x and y, we can choose to update one only, by specifying <em>flags=XUPDATE</em> or <em>flags=YUPDATE</em>. In addition, one-sided updates can be performed by setting flags to any combination of WUPDATE, EUPDATE, SUPDATE and NUPDATE to update the west, east, north and south halos respectively. Any combination of halos may be used by adding the requisite flags, e.g: <em>flags=XUPDATE+SUPDATE</em> or <em>flags=EUPDATE+WUPDATE+SUPDATE</em> will update the east, west and south halos.<br  />
 <br  />
 If a call to <em>mpp_update_domains</em> involves at least one E-W halo and one N-S halo, the corners involved will also be updated, i.e, in the example above, the SE and SW corners will be updated.<br  />
 If <em>flags</em> is not supplied, that is equivalent to <em>flags=XUPDATE+YUPDATE</em>.<br  />
 <br  />
 The vector version is passed the <em>x</em> and <em>y</em> components of a vector field in tandem, and both are updated upon return. They are passed together to treat parity issues on various grids. For example, on a cubic sphere projection, the <em>x</em> <em>y</em> components may be interchanged when passing from an equatorial cube face to a polar face. For grids with folds, vector components change sign on crossing the fold. Paired scalar quantities can also be passed with the vector version if <em>flags=SCALAR_PAIR</em>, in which case components are appropriately interchanged, but signs are not.<br  />
 <br  />
 Special treatment at boundaries such as folds is also required for staggered grids. The following types of staggered grids are recognized:<br  />
 <br  />
 1) AGRID: values are at grid centers.<br  />
 2) BGRID_NE: vector fields are at the NE vertex of a grid cell, i.e: the array elements <em>u(i,j)and</em> <em>v(i,j)are</em> actually at (i,j;) with respect to the grid centers.<br  />
 3) BGRID_SW: vector fields are at the SW vertex of a grid cell, i.e: the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i;,j;) with respect to the grid centers<br  />
 4) CGRID_NE: vector fields are at the N and E faces of a grid cell, i.e: the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i;,j) and (i,j+&amp;#189;) with respect to the grid centers.<br  />
 5) CGRID_SW: vector fields are at the S and W faces of a grid cell, i.e: the array elements <em>u(i,j)and</em> <em>v(i,j)</em> are actually at (i;,j) and (i,j;) with respect to the grid centers.<br  />
 <br  />
 The gridtypes listed above are all available by use association as integer parameters. The scalar version of <em>mpp_update_domains</em> assumes that the values of a scalar field are always at <em>AGRID</em> locations, and no special boundary treatment is required. If vector fields are at staggered locations, the optional argument <em>gridtype</em> must be appropriately set for correct treatment at boundaries. <br  />
 It is safe to apply vector field updates to the appropriate arrays irrespective of the domain topology: if the topology requires no special treatment of vector fields, specifying <em>gridtype</em> will do no harm.<br  />
 <br  />
 <em>mpp_update_domains</em> internally buffers the date being sent and received into single messages for efficiency. A turnable internal buffer area in memory is provided for this purpose by <em>mpp_domains_mod</em>. The size of this buffer area can be set by the user by calling mpp_domains <em>mpp_domains_set_stack_size</em>.Example usage: call mpp_update_domains( field, domain, flags ) Update a 1D domain for the given field. call mpp_update_domains( fieldx, fieldy, domain, flags, gridtype ) Update a 2D domain for the given fields. </div>

<h3><a id="mpp_start_update_domains"></a>mpp_start_update_domains Interface</h3><div class="textblock">Interface to start halo updates <em>mpp_start_update_domains</em> is used to start a halo update of a domain-decomposed array on each PE. <em>MPP_TYPE_</em> can be of type <em>complex</em>, <em>integer</em>, <em>logical</em> or <em>real</em>; of 4-byte or 8-byte kind; of rank up to 5. The vector version (with two input data fields) is only present for \ereal types.<br  />
 <br  />
 \empp_start_update_domains must be paired together with \empp_complete_update_domains. In <em>mpp_start_update_domains</em>, a buffer will be pre-post to receive (non-blocking) the data and data on computational domain will be packed and sent (non-blocking send) to other processor. In <em>mpp_complete_update_domains</em>, buffer will be unpacked to fill the halo and mpp_sync_self will be called to to ensure communication safe at the last call of mpp_complete_update_domains.<br  />
 <br  />
 Each mpp_update_domains can be replaced by the combination of mpp_start_update_domains and mpp_complete_update_domains. The arguments in mpp_start_update_domains and mpp_complete_update_domains should be the exact the same as in mpp_update_domains to be replaced except no optional argument "complete". The following are examples on how to replace mpp_update_domains with mpp_start_update_domains/mpp_complete_update_domains<dl class="section user"><dt>Example 1: Replace one scalar mpp_update_domains.</dt><dd><br  />
 Replace<br  />
 <br  />
 call mpp_update_domains(data, domain, flags=update_flags)<br  />
</dd></dl>
<p>with<br  />
 <br  />
 id_update = mpp_start_update_domains(data, domain, flags=update_flags)<br  />
 ...( doing some computation )<br  />
 call mpp_complete_update_domains(id_update, data, domain, flags=update_flags)<br  />
<dl class="section user"><dt>Example 2: Replace group scalar mpp_update_domains</dt><dd><br  />
 Replace<br  />
 <br  />
 call mpp_update_domains(data_1, domain, flags=update_flags, complete=.false.)<br  />
 .... ( other n-2 call mpp_update_domains with complete = .false. )<br  />
 call mpp_update_domains(data_n, domain, flags=update_flags, complete=.true. )<br  />
 <br  />
 With<br  />
 <br  />
 id_up_1 = mpp_start_update_domains(data_1, domain, flags=update_flags)<br  />
 .... ( other n-2 call mpp_start_update_domains )<br  />
 id_up_n = mpp_start_update_domains(data_n, domain, flags=update_flags)<br  />
 <br  />
 ..... ( doing some computation )<br  />
 <br  />
 call mpp_complete_update_domains(id_up_1, data_1, domain, flags=update_flags)<br  />
 .... ( other n-2 call mpp_complete_update_domains )<br  />
 call mpp_complete_update_domains(id_up_n, data_n, domain, flags=update_flags)<br  />
</dd></dl>
<dl class="section user"><dt>Example 3: Replace group CGRID_NE vector, mpp_update_domains</dt><dd><br  />
 Replace<br  />
 <br  />
 call mpp_update_domains(u_1, v_1, domain, flags=update_flgs, gridtype=CGRID_NE, complete=.false.)<br  />
 .... ( other n-2 call mpp_update_domains with complete = .false. )<br  />
 call mpp_update_domains(u_1, v_1, domain, flags=update_flags, gridtype=CGRID_NE, complete=.true. )<br  />
 <br  />
 with<br  />
 <br  />
 id_up_1 = mpp_start_update_domains(u_1, v_1, domain, flags=update_flags, gridtype=CGRID_NE)<br  />
 .... ( other n-2 call mpp_start_update_domains )<br  />
 id_up_n = mpp_start_update_domains(u_n, v_n, domain, flags=update_flags, gridtype=CGRID_NE)<br  />
 <br  />
 ..... ( doing some computation )<br  />
 <br  />
 call mpp_complete_update_domains(id_up_1, u_1, v_1, domain, flags=update_flags, gridtype=CGRID_NE)<br  />
 .... ( other n-2 call mpp_complete_update_domains )<br  />
 call mpp_complete_update_domains(id_up_n, u_n, v_n, domain, flags=update_flags, gridtype=CGRID_NE)<br  />
 <br  />
 For 2D domain updates, if there are halos present along both <em>x</em> and <em>y</em>, we can choose to update one only, by specifying <em>flags=XUPDATE</em> or <em>flags=YUPDATE</em>. In addition, one-sided updates can be performed by setting <em>flags</em> to any combination of <em>WUPDATE</em>, <em>EUPDATE</em>, <em>SUPDATE</em> and <em>NUPDATE</em>, to update the west, east, north and south halos respectively. Any combination of halos may be used by adding the requisite flags, e.g: <em>flags=XUPDATE+SUPDATE</em> or <em>flags=EUPDATE+WUPDATE+SUPDATE</em> will update the east, west and south halos.<br  />
 <br  />
 If a call to <em>mpp_start_update_domains/mpp_complete_update_domains</em> involves at least one E-W halo and one N-S halo, the corners involved will also be updated, i.e, in the example above, the SE and SW corners will be updated.<br  />
 <br  />
 If <em>flags</em> is not supplied, that is equivalent to <em>flags=XUPDATE+YUPDATE</em>.<br  />
 <br  />
 The vector version is passed the <em>x</em> and <em>y</em> components of a vector field in tandem, and both are updated upon return. They are passed together to treat parity issues on various grids. For example, on a cubic sphere projection, the <em>x</em> and <em>y</em> components may be interchanged when passing from an equatorial cube face to a polar face. For grids with folds, vector components change sign on crossing the fold. Paired scalar quantities can also be passed with the vector version if flags=SCALAR_PAIR, in which case components are appropriately interchanged, but signs are not.<br  />
 <br  />
 Special treatment at boundaries such as folds is also required for staggered grids. The following types of staggered grids are recognized: <br  />
 1) <em>AGRID:</em> values are at grid centers.<br  />
 2) <em>BGRID_NE:</em> vector fields are at the NE vertex of a grid cell, i.e: the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i+&amp;#189;,j+&amp;#189;) with respect to the grid centers.<br  />
 3) <em>BGRID_SW:</em> vector fields are at the SW vertex of a grid cell, i.e., the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i-&amp;#189;,j-&amp;#189;) with respect to the grid centers.<br  />
 4) <em>CGRID_NE:</em> vector fields are at the N and E faces of a grid cell, i.e: the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i+&amp;#189;,j) and (i,j+&amp;#189;) with respect to the grid centers.<br  />
 5) <em>CGRID_SW:</em> vector fields are at the S and W faces of a grid cell, i.e: the array elements <em>u(i,j)</em> and <em>v(i,j)</em> are actually at (i-&amp;#189;,j) and (i,j-&amp;#189;) with respect to the grid centers.<br  />
 <br  />
 The gridtypes listed above are all available by use association as integer parameters. If vector fields are at staggered locations, the optional argument <em>gridtype</em> must be appropriately set for correct treatment at boundaries. <br  />
 It is safe to apply vector field updates to the appropriate arrays irrespective of the domain topology: if the topology requires no special treatment of vector fields, specifying <em>gridtype</em> will do no harm.<br  />
 <br  />
 <em>mpp_start_update_domains/mpp_complete_update_domains</em> internally buffers the data being sent and received into single messages for efficiency. A turnable internal buffer area in memory is provided for this purpose by <em>mpp_domains_mod</em>. The size of this buffer area can be set by the user by calling <em>mpp_domains_set_stack_size</em>.<br  />
 Example usage: call mpp_start_update_domains( field, domain, flags ) call mpp_complete_update_domains( field, domain, flags ) </dd></dl>
</div>

<h3><a id="mpp_complete_update_domains"></a>mpp_complete_mpp_domains Interface</h3><div class="textblock">Must be used after a call to <a class="el" href="group__mpp__domains__mod.html">mpp_start_update_domains Interface</a> in order to complete a nonblocking domain update. See <a class="el" href="group__mpp__domains__mod.html">mpp_start_update_domains Interface</a> for more info. </div>

<h3><a id="mpp_update_nest_fine"></a>mpp_update_nest_fine Interface</h3><div class="textblock">Pass the data from coarse grid to fill the buffer to be ready to be interpolated nto fine grid. <br  />
Example usage: call mpp_update_nest_fine(field, nest_domain, wbuffer, ebuffer, sbuffer, nbuffer, nest_level, flags, complete, position, extra_halo, name, tile_count) </div>

<h3><a id="mpp_update_nest_coarse"></a>mpp_update_nest_coarse Interface</h3><div class="textblock">Pass the data from fine grid to fill the buffer to be ready to be interpolated onto coarse grid. <br  />
Example usage: call mpp_update_nest_coarse(field, nest_domain, field_out, nest_level, complete, position, name, tile_count) </div>

<h3><a id="mpp_get_F2C_index"></a>mpp_get_F2C_index Interface</h3><div class="textblock">Get the index of the data passed from fine grid to coarse grid. <br  />
Example usage: call mpp_get_F2C_index(nest_domain, is_coarse, ie_coarse, js_coarse, je_coarse, is_fine, ie_fine, js_fine, je_fine, nest_level, position) </div>

<h3><a id="mpp_broadcast_domain"></a>mpp_broadcast_domain Interface</h3><div class="textblock">Send domain to every pe </div>

<h3><a id="mpp_update_domains_ad"></a>mpp_update_domains_ad Interface</h3><div class="textblock">Similar to <a class="el" href="group__mpp__domains__mod.html">mpp_update_domains Interface</a> , updates adjoint domains </div>

<h3><a id="mpp_pass_SG_to_UG"></a>mpp_pass_SG_to_UG</h3><div class="textblock">Passes data from a structured grid to an unstructured grid <br  />
Example usage: call mpp_pass_SG_to_UG(domain, sg_data, ug_data) </div>

<h3><a id="mpp_get_boundary"></a>mpp_get_boundary Interface</h3><div class="textblock">Get the boundary data for symmetric domain when the data is at C, E, or N-cell center.<br  />
 <em>mpp_get_boundary</em> is used to get the boundary data for symmetric domain when the data is at C, E, or N-cell center. For cubic grid, the data should always at C-cell center. <br  />
Example usage: call mpp_get_boundary(domain, field, ebuffer, sbuffer, wbuffer, nbuffer) Get boundary information from domain and field and store in buffers </div>

<h3><a id="mpp_redistribute"></a>mpp_redistribute Interface</h3><div class="textblock">Reorganization of distributed global arrays.<br  />
 <em>mpp_redistribute</em> is used to reorganize a distributed array. <em>MPP_TYPE_can</em> be of type <em>integer</em>, <em>complex</em>, or <em>real</em>; of 4-byte or 8-byte kind; of rank up to 5. <br  />
Example usage: call mpp_redistribute( domain_in, field_in, domain_out, field_out ) </div>

<h3><a id="mpp_check_field"></a>mpp_check_field Interface</h3><div class="textblock">Parallel checking between two ensembles which run on different set pes at the same time<br  />
 There are two forms for the <code>mpp_check_field</code> call. The 2D version is generally to be used and 3D version is built by repeated calls to the 2D version.<br  />
 <br  />
Example usage: call mpp_check_field(field_in, pelist1, pelist2, domain, mesg, &amp; w_halo, s_halo, e_halo, n_halo, force_abort ) </div>

<h3><a id="mpp_global_field"></a>mpp_global_field Interface</h3><div class="textblock">Fill in a global array from domain-decomposed arrays.<br  />
<code>mpp_global_field</code> is used to get an entire domain-decomposed array on each PE. <code>MPP_TYPE_</code> can be of type <code>complex</code>, <code>integer</code>, <code>logical</code> or <code>real</code>; of 4-byte or 8-byte kind; of rank up to 5.<br  />
All PEs in a domain decomposition must call <code>mpp_global_field</code>, and each will have a complete global field at the end. Please note that a global array of rank 3 or higher could occupy a lot of memory. </div>

<h3><a id="mpp_global_max"></a>mpp_global_max Interface</h3><div class="textblock">Global max/min of domain-decomposed arrays.<br  />
 <em>mpp_global_max</em> is used to get the maximum value of a domain-decomposed array on each PE. <em>MPP_TYPE_can</em> be of type <em>integer</em> or <em>real</em>; of 4-byte or 8-byte kind; of rank up to 5. The dimension of <em>locus</em> must equal the rank of <em>field</em>.<br  />
 <br  />
 All PEs in a domain decomposition must call <em>mpp_global_max</em>, and each will have the result upon exit. The function <em>mpp_global_min</em>, with an identical syntax. is also available.<br  />
Example usage: mpp_global_max( domain, field, locus ) </div>

<h3><a id="mpp_global_sum"></a>mpp_global_sum Interface</h3><div class="textblock">Global sum of domain-decomposed arrays.<br  />
 <em>mpp_global_sum</em> is used to get the sum of a domain-decomposed array on each PE. <em>MPP_TYPE_</em> can be of type <em>integer</em>, <em>complex</em>, or <em>real</em>; of 4-byte or 8-byte kind; of rank up to 5. <br  />
Example usage: call mpp_global_sum( domain, field, flags ) </p><dl class="section note"><dt>Note</dt><dd>All PEs in a domain decomposition must call <em>mpp_global_sum</em>, and each will have the result upon exit. </dd></dl>
</div>

<h3><a id="mpp_get_neighbor_pe"></a>mpp_get_neighbor_pe Interface</h3><div class="textblock">Retrieve PE number of a neighboring domain.Given a 1-D or 2-D domain decomposition, this call allows users to retrieve the PE number of an adjacent PE-domain while taking into account that the domain may have holes (masked) and/or have cyclic boundary conditions and/or a folded edge. Which PE-domain will be retrived will depend on "direction": +1 (right) or -1 (left) for a 1-D domain decomposition and either NORTH, SOUTH, EAST, WEST, NORTH_EAST, SOUTH_EAST, SOUTH_WEST, or NORTH_WEST for a 2-D decomposition. If no neighboring domain exists (masked domain), then the returned "pe" value will be set to NULL_PE.<br  />
 <br  />
Example usage: call mpp_get_neighbor_pe( domain1d, direction=+1 , pe) Set pe to the neighbor pe number that is to the right of the current pe call mpp_get_neighbor_pe( domain2d, direction=NORTH, pe) Get neighbor pe number that's above/north of the current pe </div>

<h3><a id="operators_domains"></a>Operator Overrides for domain types</h3><div class="textblock">Equality/inequality operators for domaintypes. <br  />
<br  />
The module provides public operators to check for equality/inequality of domaintypes, e.g:<br  />
 </p><pre class="fragment">       type(domain1D) :: a, b&lt;br&gt;
       type(domain2D) :: c, d&lt;br&gt;
       ...&lt;br&gt;
       if( a.NE.b )then&lt;br&gt;
       ...&lt;br&gt;
       end if&lt;br&gt;
       if( c==d )then&lt;br&gt;
       ...&lt;br&gt;
       end if&lt;br&gt;
</pre><p> <br  />
 Domains are considered equal if and only if the start and end indices of each of their component global, data and compute domains are equal. </div>

<h3><a id="mpp_get_compute_domain"></a>mpp_get_compute_domain Interface</h3><div class="textblock">These routines retrieve the axis specifications associated with the compute domains. The domain is a derived type with private elements. These routines retrieve the axis specifications associated with the compute domains The 2D version of these is a simple extension of 1D. <br  />
Example usage: call mpp_get_compute_domain(domain_1D, is, ie) call mpp_get_compute_domain(domain_2D, is, ie, js, je) </div>

<h3><a id="mpp_get_compute_domains"></a>mpp_get_compute_domains Interface</h3><div class="textblock">Retrieve the entire array of compute domain extents associated with a decomposition. <br  />
Example usage: call mpp_get_compute_domains( domain, xbegin, xend, xsize, &amp;<br  />
 ybegin, yend, ysize ) </div>

<h3><a id="mpp_get_data_domain"></a>mpp_get_data_domain Interface</h3><div class="textblock">These routines retrieve the axis specifications associated with the data domains. The domain is a derived type with private elements. These routines retrieve the axis specifications associated with the data domains. The 2D version of these is a simple extension of 1D. <br  />
Example usage: call mpp_get_data_domain(domain_1d, isd, ied) call mpp_get_data_domain(domain_2d, isd, ied, jsd, jed) </div>

<h3><a id="mpp_get_global_domain"></a>mpp_get_global_domain Interface</h3><div class="textblock">These routines retrieve the axis specifications associated with the global domains. The domain is a derived type with private elements. These routines retrieve the axis specifications associated with the global domains. The 2D version of these is a simple extension of 1D. <br  />
Example usage: call mpp_get_global_domain(domain_1d, isg, ieg) call mpp_get_global_domain(domain_2d, isg, ieg, jsg, jeg) </div>

<h3><a id="mpp_get_memory_domain"></a>mpp_get_memory_domain Interface</h3><div class="textblock">These routines retrieve the axis specifications associated with the memory domains. The domain is a derived type with private elements. These routines retrieve the axis specifications associated with the memory domains. The 2D version of these is a simple extension of 1D. <br  />
Example usage: call mpp_get_memory_domain(domain_1d, ism, iem) call mpp_get_memory_domain(domain_2d, ism, iem, jsm, jem) </div>

<h3><a id="mpp_set_compute_domain"></a>mpp_set_compute_domain Interface</h3><div class="textblock">These routines set the axis specifications associated with the compute domains. The domain is a derived type with private elements. These routines set the axis specifications associated with the compute domains The 2D version of these is a simple extension of 1D. <br  />
Example usage: call mpp_set_compute_domain() </div>

<h3><a id="mpp_set_data_domain"></a>mpp_set_data_domain Interface</h3><div class="textblock">These routines set the axis specifications associated with the data domains. The domain is a derived type with private elements. These routines set the axis specifications associated with the data domains. The 2D version of these is a simple extension of 1D. </div>

<h3><a id="mpp_set_global_domain"></a>mpp_set_global_domain Interface</h3><div class="textblock">These routines set the axis specifications associated with the global domains. The domain is a derived type with private elements. These routines set the axis specifications associated with the global domains. The 2D version of these is a simple extension of 1D. </div>

<h3><a id="mpp_get_pelist"></a>mpp_get_pelist Interface</h3><div class="textblock">Retrieve list of PEs associated with a domain decomposition. The 1D version of this call returns an array of the PEs assigned to this 1D domain decomposition. In addition the optional argument pos may be used to retrieve the 0-based position of the domain local to the calling PE, i.e., <em>domainlist</em>(pos)pe is the local PE, as returned by <a class="el" href="mpp__util_8inc.html#acfeba7de56f9dd6000b3fd78f9d56c50" title="Returns processor ID. This returns the unique ID associated with a PE. This number runs between 0 and...">mpp_pe()</a> The 2D version of this call is identical to 1D version. </div>

<h3><a id="mpp_get_layout"></a>mpp_get_layout Interface</h3><div class="textblock">Retrieve layout associated with a domain decomposition The 1D version of this call returns the number of divisions that was assigned to this decomposition axis. The 2D version of this call returns an array of dimension 2 holding the results on two axes. <br  />
Example usage: call mpp_get_layout( domain, layout ) </div>

<h3><a id="mpp_nullify_domain_list"></a>mpp_nullify_domain_list Interface</h3><div class="textblock">Nullify domain list. This interface is needed in mpp_domains_test. 1-D case can be added in if needed. Example usage: call mpp_nullify_domain_list(domain) </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
